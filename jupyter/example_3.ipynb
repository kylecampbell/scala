{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  \n",
    "Write a function that will put N doubles into a file. The doubles need to be normally distributed\n",
    "with mean 0 and standard deviation 1. The function should have two arguments: N\n",
    "and the full name of the file (ie includes path to file location).  \n",
    "  \n",
    "Create a file with 50,000 doubles using the function from problem 1. This file will be used for\n",
    "the next several problems. It is best if you put the file in the current directory to avoid paths that\n",
    "do not exist on other machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newFile(N: Int, filepath: String) = {\n",
    "    val arr1 = Array.fill(N)(scala.util.Random.nextGaussian())\n",
    "    val avg = arr1.sum/N\n",
    "    val std = scala.math.sqrt( arr1.map(x => scala.math.pow((x-avg),2)).sum / (N-1) )\n",
    "    \n",
    "    scala.tools.nsc.io.File(filepath).writeAll(arr1.mkString(\"\\n\"))\n",
    "    \n",
    "    assert(arr1.length == N)\n",
    "    assert( (avg < 0.1) && (avg > -0.1) )\n",
    "    assert( (std >= 0.9) && (std <= 1.1) )\n",
    "    println(\"mean: \" + avg)\n",
    "    println(\"stdv: \" + std)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -8.260468405144706E-4\n",
      "stdv: 1.0003023769931492\n"
     ]
    }
   ],
   "source": [
    "newFile(50000, \"file1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  \n",
    "Read the file created in #2 into an RDD and compute the mean and standard deviation of\n",
    "the doubles in the file. Work on the RDD, that is do not convert the RDD to a DataFrame or\n",
    "Dataset.. You are to use Spark code to compute the values as we want this to run on a\n",
    "cluster using multiple machines. So the pure Scala code you used in assignment will not\n",
    "work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -8.26046840514488E-4\n",
      "stdev: 1.0002923739193694\n"
     ]
    }
   ],
   "source": [
    "val source1 = scala.io.Source.fromFile(\"file1.txt\", \"UTF-8\")\n",
    "val tokens1 = source1.mkString.split(\"\\\\s+\")\n",
    "val nums1 = tokens1.map(_.toDouble)\n",
    "val rdd = sc.parallelize(nums1)\n",
    "val mean1 = rdd.mean()\n",
    "val stdev1 = rdd.stdev()\n",
    "println(\"mean: \" + mean1)\n",
    "println(\"stdev: \" + stdev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  \n",
    "Repeat #3 but using a DataFrame instead of RDD. Here work on the DataFrame not an\n",
    "RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          avg(value)|\n",
      "+--------------------+\n",
      "|-8.26046840514470...|\n",
      "+--------------------+\n",
      "\n",
      "+------------------+\n",
      "|stddev_samp(value)|\n",
      "+------------------+\n",
      "| 1.000302376993155|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val reader = spark.read\n",
    "reader.option(\"header\",false)\n",
    "reader.option(\"inferSchema\",false)\n",
    "reader.option(\"sep\",\"\\n\")\n",
    "val df = reader.text(\"file1.txt\")\n",
    "\n",
    "df.select(mean(df(\"value\"))).show()\n",
    "df.select(stddev(df(\"value\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  \n",
    "Using a DataFrame create a random sample of about 100 elements of the file created in #2\n",
    "and compute the mean of the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 100/50000 = 0.002\n",
    "val dfSample = df.sample(true, 0.002).limit(100)\n",
    "dfSample.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(value)|\n",
      "+-----------------+\n",
      "|0.110678406963615|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSample.select(mean(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  \n",
    "Create a file of 100 normally distributed doubles. Read the doubles from the file into an\n",
    "RDD. Using the RDD create a sliding window of size 20 and compute the mean of each\n",
    "window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-20:   0.08583182707064407\n",
      "21-40:  -0.23986637832375474\n",
      "41-60:  -0.22226077556934193\n",
      "61-80:  -0.0940150257002612\n",
      "81-100: 0.33477403255179883\n"
     ]
    }
   ],
   "source": [
    "val df100 = spark.range(100).select(randn()).toDF(\"doubles\")\n",
    "\n",
    "val filePath = \"df100.csv\"\n",
    "df100.coalesce(1).write.option(\"header\", \"false\").csv(\"df100.csv\")\n",
    "\n",
    "val fp = \"df100.csv/part-00000-2094aff9-b999-4477-aaf7-7a8971dfa5bb-c000.csv\"\n",
    "val rdd100 = sc.textFile(fp).flatMap(_.split(\"\\t\")).map(_.toDouble).cache\n",
    "\n",
    "val pairs = rdd100.zipWithIndex\n",
    "val slide1 = pairs.filter{case (v,k) => k<20}.keys.mean\n",
    "val slide2 = pairs.filter{case (v,k) => k>19 && k<40}.keys.mean\n",
    "val slide3 = pairs.filter{case (v,k) => k>39 && k<60}.keys.mean\n",
    "val slide4 = pairs.filter{case (v,k) => k>59 && k<80}.keys.mean\n",
    "val slide5 = pairs.filter{case (v,k) => k>79}.keys.mean\n",
    "\n",
    "assert(rdd100.count == 100)\n",
    "assert(pairs.filter{case (v,k) => k<20}.keys.count == 20)\n",
    "assert(pairs.filter{case (v,k) => k>19 && k<40}.keys.count == 20)\n",
    "assert(pairs.filter{case (v,k) => k>39 && k<60}.keys.count == 20)\n",
    "assert(pairs.filter{case (v,k) => k>59 && k<80}.keys.count == 20)\n",
    "assert(pairs.filter{case (v,k) => k>79}.keys.count == 20)\n",
    "\n",
    "println(\"1-20:   \" + slide1)\n",
    "println(\"21-40:  \" + slide2)\n",
    "println(\"41-60:  \" + slide3)\n",
    "println(\"61-80:  \" + slide4)\n",
    "println(\"81-100: \" + slide5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  \n",
    "The file “multiple-sites.tsv” contains two columns: site and dwell-time. Using Spark compute\n",
    "the average dwell time for each site.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "var msDf = spark.read.format(\"csv\").\n",
    "                option(\"header\", \"true\").\n",
    "                option(\"delimiter\", \"\\t\").\n",
    "                load(\"multiple-sites.tsv\")\n",
    "\n",
    "println(msDf.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|site|          avg-time|\n",
      "+----+------------------+\n",
      "|   7|123.36734693877551|\n",
      "|  15|119.34782608695652|\n",
      "|  11| 96.98214285714286|\n",
      "|   3| 97.47916666666667|\n",
      "|   8| 94.34693877551021|\n",
      "|  16| 86.74418604651163|\n",
      "|   0| 79.85106382978724|\n",
      "|   5|102.33333333333333|\n",
      "|  18| 94.81481481481481|\n",
      "|  17|  77.8913043478261|\n",
      "+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msDf.groupBy(\"site\").agg(mean(\"dwell-time\") as \"avg-time\").show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.  \n",
    "The file “multiple-sites.tsv” contains two columns: date and dwell-time. Using Spark compute  \n",
    "the following:  \n",
    "1. The average dwell time each hour  \n",
    "2. The average dwell time per day of week  \n",
    "3. The average dwell time on week-days (Monday - Friday)  \n",
    "4. Average dwell time on the weekend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var dtDf = spark.read.format(\"csv\").\n",
    "                option(\"header\", \"true\").\n",
    "                option(\"delimiter\", \"\\t\").\n",
    "                load(\"dwell-times.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Average dwell time each hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|              window|         avg-hour|\n",
      "+--------------------+-----------------+\n",
      "|[2015-01-05 08:00...|          83.8125|\n",
      "|[2015-01-09 12:00...|           81.375|\n",
      "|[2015-01-15 12:00...|80.52941176470588|\n",
      "|[2015-01-24 01:00...|             35.0|\n",
      "|[2015-01-28 04:00...|92.26666666666667|\n",
      "|[2015-02-06 02:00...|             63.0|\n",
      "|[2015-02-15 11:00...|95.77777777777777|\n",
      "|[2015-02-18 14:00...|71.36363636363636|\n",
      "|[2015-03-01 20:00...|             65.0|\n",
      "|[2015-03-04 00:00...|          111.125|\n",
      "+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtDf.groupBy(window(dtDf.col(\"date\"),\"1 hour\")).agg(mean(\"dwell-time\") as \"avg-hour\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Average dwell time per day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              window|           avg-day|\n",
      "+--------------------+------------------+\n",
      "|[2015-03-01 00:00...|116.47272727272727|\n",
      "|[2015-01-24 00:00...|112.73076923076923|\n",
      "|[2015-01-26 00:00...| 92.41025641025641|\n",
      "|[2015-02-18 00:00...| 81.37800687285224|\n",
      "|[2015-03-11 00:00...|  87.7127659574468|\n",
      "|[2015-05-14 00:00...| 96.94561186650185|\n",
      "|[2015-02-17 00:00...| 85.50541516245487|\n",
      "|[2015-03-06 00:00...| 91.01186943620178|\n",
      "|[2015-01-03 00:00...|            124.66|\n",
      "|[2015-04-06 00:00...|      88.935546875|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtDf.groupBy(window(dtDf.col(\"date\"),\"1 day\")).agg(mean(\"dwell-time\") as \"avg-day\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Average dwell time on week-days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+---------+-----------------+\n",
      "|      Day|  avg-day-of-week|\n",
      "+---------+-----------------+\n",
      "|Wednesday|90.74125065685759|\n",
      "|  Tuesday|88.99967886962106|\n",
      "|   Friday|88.68935445068163|\n",
      "| Thursday|91.99423893268647|\n",
      "|   Monday|90.60352703707639|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val week = dtDf.withColumn(\"Day\", date_format(col(\"date\"),\"EEEE\")).groupBy(\"Day\").agg(mean(\"dwell-time\") as \"avg-day-of-week\")\n",
    "week.where(\"Day == 'Monday' or Day == 'Tuesday' or Day == 'Wednesday' or Day == 'Thursday' or Day == 'Friday'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Average dwell time on the weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+--------+------------------+\n",
      "|     Day|   avg-day-of-week|\n",
      "+--------+------------------+\n",
      "|Saturday|118.96697187704382|\n",
      "|  Sunday|116.49892933618844|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "week.where(\"Day == 'Saturday' or Day == 'Sunday'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.  \n",
    "Do the average dwell times computed in #7 indicate any difference in users behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dwell times are predominantly higher during the weekends than the weekdays. This makes sense as it is more  \n",
    "probable for people to have more free time during the weekend where Saturday reaches the maximum while  \n",
    "Tuesday and Friday are the low points.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
